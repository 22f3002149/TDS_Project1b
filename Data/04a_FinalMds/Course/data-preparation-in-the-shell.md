## Data Preparation in the Shell

[![Data preparation in the shell](https://i.ytimg.com/vi_webp/XEdy4WK70vU/sddefault.webp)

This image captures a Google search for "DataForSEO" in dark mode. It shows the company's website (`dataforseo.com`) with its logo and headline promoting an "API Stack for Data-Driven SEO Tools". Key sections highlight their function as an SEO data source and provide pricing information, alongside social media links. A prominent "People also ask" section includes questions about usage, availability of free versions, SERP meaning, and the nature of their ranking API.
](https://youtu.be/XEdy4WK70vU)

You'll learn how to use UNIX tools to process and clean data, covering:

- `curl` (or `wget`) to fetch data from websites.
- `gzip` (or `xz`) to compress and decompress files.
- `wc` to count lines, words, and characters in text.
- `head` and `tail` to get the start and end of files.
- `cut` to extract specific columns from text.
- `uniq` to de-duplicate lines.
- `sort` to sort lines.
- `grep` to filter lines containing specific text.
- `sed` to search and replace text.
- `awk` for more complex text processing.

Here are the links used in the video:

- [Data preparation in the shell - Notebook](https://colab.research.google.com/drive/1KSFkQDK0v__XWaAaHKeQuIAwYV0dkTe8)
- [Data Science at the Command Line](https://jeroenjanssens.com/dsatcl/)
